{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd880d06-4a83-4191-ae62-59eca676a84d",
      "metadata": {
        "id": "dd880d06-4a83-4191-ae62-59eca676a84d"
      },
      "outputs": [],
      "source": [
        "# Hate is tagged with 1 and NO-hate with 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "142c1fbc-73c0-4759-b277-c8885a0f71e9",
      "metadata": {
        "id": "142c1fbc-73c0-4759-b277-c8885a0f71e9"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Paths of the directory\n",
        "PROJECT_ROOT_DIR = os.getcwd()\n",
        "PROJECT_ROOT_DIR = os.path.join(PROJECT_ROOT_DIR, \"proyecto\")\n",
        "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\")\n",
        "DATA_PATH = os.path.join(PROJECT_ROOT_DIR, \"datasets\")\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "# Where to save the figures\n",
        "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
        "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
        "    print(\"Saving figure\", fig_id)\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
        "\n",
        "# Where to unzip data\n",
        "os.makedirs(DATA_PATH, exist_ok=True)\n",
        "\n",
        "def unzip_data(zipfile_name, directory_to_extract_to = DATA_PATH):\n",
        "\n",
        "    path_to_zip_file = os.path.join(PROJECT_ROOT_DIR, zipfile_name)\n",
        "\n",
        "    with zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:\n",
        "        zip_ref.extractall(directory_to_extract_to)\n",
        "\n",
        "unzip_data(os.path.join(PROJECT_ROOT_DIR, \"pan21-author-profiling-test-2021-04-12.zip\"))\n",
        "unzip_data(os.path.join(PROJECT_ROOT_DIR, \"pan21-author-profiling-training-2021-03-14.zip\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "814f7df2-d6eb-4809-8388-a4a7fe1d9230",
      "metadata": {
        "id": "814f7df2-d6eb-4809-8388-a4a7fe1d9230"
      },
      "outputs": [],
      "source": [
        "# Data processing\n",
        "\n",
        "import pandas as pd\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "L = [{'user' : [], 'tweets' : [], 'language' : [], 'class' : []}, {'user' : [], 'tweets' : [], 'language' : [], 'class' : []}]    # A list of two components with dictionaries in each of them with which we will create the dataframes, in the first component the training set and in the second one the test set\n",
        "paths_list = [DATA_PATH]    # A list with the directories we will iterate, the last component is the current directory and the previous ones the ones we have already iterated to reach the current one\n",
        "\n",
        "for dir1 in os.listdir(paths_list[0]):    # os.listdir(path) returns a list of the names of the files inside the given path\n",
        "    if dir1 == 'pan21-author-profiling-training-2021-03-14':\n",
        "        set_type = 0    # The training set will be in the first component\n",
        "    elif dir1 == 'pan21-author-profiling-test-2021-04-12':\n",
        "        set_type = 1    # The test set will be in the second component\n",
        "    else:    # In order to skip the file '.ipynb_checkpoints' created by Jupyter\n",
        "        continue\n",
        "    paths_list.append(os.path.join(paths_list[-1], dir1))\n",
        "    for dir2 in os.listdir(paths_list[-1]):\n",
        "        if dir2 == \"en\":\n",
        "            lng = 'english'\n",
        "        else:\n",
        "            lng = 'spanish'\n",
        "        paths_list.append(os.path.join(paths_list[-1], dir2))\n",
        "        for user in os.listdir(paths_list[-1]):\n",
        "            if user == '.ipynb_checkpoints' or user == 'truth.txt':\n",
        "                continue\n",
        "            tree = ET.parse(os.path.join(paths_list[-1], user))\n",
        "            root = tree.getroot()\n",
        "\n",
        "            tweets = []\n",
        "\n",
        "            cls = int(root.attrib['class'])    # The root attributes are a dictionary and the value of the key 'class' is a string with a '0' (no hate speech) or a '1' (yes hate speech)\n",
        "\n",
        "            for documents in root:\n",
        "                for document in documents:\n",
        "                    tweets.append(document.text)\n",
        "            L[set_type]['user'].append(user)\n",
        "            L[set_type]['tweets'].append(tweets)\n",
        "            L[set_type]['language'].append(lng)\n",
        "            L[set_type]['class'].append(cls)\n",
        "        del paths_list[-1]\n",
        "    del paths_list[-1]\n",
        "\n",
        "training_set = pd.DataFrame(L[0])\n",
        "test_set = pd.DataFrame(L[1])\n",
        "\n",
        "training_set.to_csv(os.path.join(DATA_PATH, \"training_set.csv\"), index=False)\n",
        "test_set.to_csv(os.path.join(DATA_PATH, \"test_set.csv\"), index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53d0fed6-f7db-4a22-9018-1cfae7c57e00",
      "metadata": {
        "id": "53d0fed6-f7db-4a22-9018-1cfae7c57e00",
        "outputId": "805d6543-9856-4bcc-e13f-ab08572df15e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user</th>\n",
              "      <th>tweets</th>\n",
              "      <th>language</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>d2e0f4f0244b9b8b3bbd8b1654be5b74.xml</td>\n",
              "      <td>[I‚Äôm just being me under construction #HASHTAG...</td>\n",
              "      <td>english</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>f30abea44b6c4144c0690f98459428a6.xml</td>\n",
              "      <td>[i was locked up sending you roses, acabei de ...</td>\n",
              "      <td>english</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>d28b60028cf7bcf8a9f145193e261ecf.xml</td>\n",
              "      <td>[This crippled goob saw the ‚Äúanti-Semitic plat...</td>\n",
              "      <td>english</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>78537787441ed0d11da43122f9b0520a.xml</td>\n",
              "      <td>[RT #USER#: EXCLUSIVE: 99% chance COVID-19 was...</td>\n",
              "      <td>english</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>30be488aa93e8000aadb952a9cd5143c.xml</td>\n",
              "      <td>[#USER# Yes, yes she did, RT #USER#: What are ...</td>\n",
              "      <td>english</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   user  \\\n",
              "0  d2e0f4f0244b9b8b3bbd8b1654be5b74.xml   \n",
              "1  f30abea44b6c4144c0690f98459428a6.xml   \n",
              "2  d28b60028cf7bcf8a9f145193e261ecf.xml   \n",
              "3  78537787441ed0d11da43122f9b0520a.xml   \n",
              "4  30be488aa93e8000aadb952a9cd5143c.xml   \n",
              "\n",
              "                                              tweets language  class  \n",
              "0  [I‚Äôm just being me under construction #HASHTAG...  english      0  \n",
              "1  [i was locked up sending you roses, acabei de ...  english      0  \n",
              "2  [This crippled goob saw the ‚Äúanti-Semitic plat...  english      1  \n",
              "3  [RT #USER#: EXCLUSIVE: 99% chance COVID-19 was...  english      1  \n",
              "4  [#USER# Yes, yes she did, RT #USER#: What are ...  english      0  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "training_set.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a420fa9b-bdac-42aa-8218-9aa391ef9d94",
      "metadata": {
        "id": "a420fa9b-bdac-42aa-8218-9aa391ef9d94",
        "outputId": "2548832f-686a-4bef-eb88-c6c7a1d8ae43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 400 entries, 0 to 399\n",
            "Data columns (total 4 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   user      400 non-null    object\n",
            " 1   tweets    400 non-null    object\n",
            " 2   language  400 non-null    object\n",
            " 3   class     400 non-null    int64 \n",
            "dtypes: int64(1), object(3)\n",
            "memory usage: 12.6+ KB\n"
          ]
        }
      ],
      "source": [
        "training_set.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6a535cf-31aa-4faf-96fd-a249062089cb",
      "metadata": {
        "id": "a6a535cf-31aa-4faf-96fd-a249062089cb",
        "outputId": "4f49dbb5-8b88-4a25-afcc-3e7b368afc88"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0      [I‚Äôm just being me under construction #HASHTAG...\n",
              "1      [i was locked up sending you roses, acabei de ...\n",
              "2      [This crippled goob saw the ‚Äúanti-Semitic plat...\n",
              "3      [RT #USER#: EXCLUSIVE: 99% chance COVID-19 was...\n",
              "4      [#USER# Yes, yes she did, RT #USER#: What are ...\n",
              "                             ...                        \n",
              "395    [#USER# De todas formas tu consejo y comentari...\n",
              "396    [Pero el bar de pueblo donde van a jugar 10 vi...\n",
              "397    [#USER# menudo repaso tu, #USER# #USER# yo tam...\n",
              "398    [Auxilio, rasa. Me cruc√© por la marcha feminis...\n",
              "399    [Trabajo de hoy üíó #URL#, Los chavos de cares e...\n",
              "Name: tweets, Length: 400, dtype: object"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "training_set['tweets']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb9994a0-de4a-4c69-a8ac-f1e4183bdd41",
      "metadata": {
        "id": "fb9994a0-de4a-4c69-a8ac-f1e4183bdd41",
        "outputId": "d92d32ec-f45a-4fc2-f6bb-90a326a55976"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['I‚Äôm just being me under construction #HASHTAG# on the way #URL#',\n",
              " 'one person followed me // automatically checked by #URL#',\n",
              " 'one person followed me // automatically checked by #URL#',\n",
              " 'one person followed me // automatically checked by #URL#',\n",
              " 'one person followed me and one person unfollowed me // automatically checked by #URL#',\n",
              " 'My birthday coming up and shit it‚Äôs lit',\n",
              " 'one person unfollowed me // automatically checked by #URL#',\n",
              " 'I don‚Äôt post on here so fuck it #URL#',\n",
              " 'one person followed me // automatically checked by #URL#',\n",
              " 'one person followed me // automatically checked by #URL#',\n",
              " 'one person followed me // automatically checked by #URL#',\n",
              " 'one person unfollowed me // automatically checked by #URL#',\n",
              " 'one person followed me // automatically checked by #URL#',\n",
              " 'one person followed me // automatically checked by #URL#',\n",
              " 'one person unfollowed me // automatically checked by #URL#',\n",
              " 'one person followed me // automatically checked by #URL#',\n",
              " 'one person followed me // automatically checked by #URL#',\n",
              " 'one person unfollowed me // automatically checked by #URL#',\n",
              " 'one person unfollowed me // automatically checked by #URL#',\n",
              " 'one person followed me // automatically checked by #URL#',\n",
              " 'Niggas ain‚Äôt trying portray a image we just trying become our best selves #URL#',\n",
              " '2 people followed me // automatically checked by #URL#',\n",
              " 'one person followed me // automatically checked by #URL#',\n",
              " 'one person followed me // automatically checked by #URL#',\n",
              " 'one person followed me // automatically checked by #URL#',\n",
              " 'one person unfollowed me // automatically checked by #URL#',\n",
              " 'one person followed me // automatically checked by #URL#',\n",
              " 'This shit be all facts üíØüíØüíØ #URL#',\n",
              " 'RT #USER#: i need a girl friend all to myself.',\n",
              " 'RT #USER#: Snippet from the song called stick play dropping on my tape called I‚Äôm just being me #URL#',\n",
              " 'one person followed me // automatically checked by #URL#',\n",
              " 'If you going do it do it 100 percent',\n",
              " 'RT #USER#: Them tables definitely turn . U gotta b as humble as possible',\n",
              " 'This shit going be a hit üíØ #URL#',\n",
              " 'one person unfollowed me // automatically checked by #URL#',\n",
              " 'I‚Äôm really finding my artistry',\n",
              " '#USER# #USER# You throwing celebrities bull shit outta proportion üòÇüíØ',\n",
              " 'Bad Idea üòÇüòÇ #URL#',\n",
              " 'I been different',\n",
              " 'Yeah I really been on my shit üíØüíØüíØ Tune in üñ§üôÖüèæ\\u200d‚ôÇÔ∏èüß¢ #URL#',\n",
              " 'Snippet from the song called stick play dropping on my tape called I‚Äôm just being me #URL#',\n",
              " '2 people unfollowed me // automatically checked by #URL#',\n",
              " 'one person followed me // automatically checked by #URL#',\n",
              " 'RT #USER#: 2020 was a brutal year üíî',\n",
              " 'Been off my grown man shit Lately getting money and staying out da way üíØüíØüíØ',\n",
              " 'Net working is really the key',\n",
              " 'Ain‚Äôt been on here in for ever üòÇ',\n",
              " 'Show me some love real quick Lls',\n",
              " 'one person followed me // automatically checked by #URL#',\n",
              " 'one person unfollowed me // automatically checked by #URL#',\n",
              " 'one person followed me // automatically checked by #URL#',\n",
              " 'one person followed me // automatically checked by #URL#',\n",
              " 'one person followed me // automatically checked by #URL#',\n",
              " 'one person unfollowed me // automatically checked by #URL#',\n",
              " '2 people followed me // automatically checked by #URL#',\n",
              " 'one person followed me and one person unfollowed me // automatically checked by #URL#',\n",
              " 'one person followed me // automatically checked by #URL#',\n",
              " 'one person unfollowed me // automatically checked by #URL#',\n",
              " 'one person followed me // automatically checked by #URL#',\n",
              " 'one person followed me // automatically checked by #URL#',\n",
              " 'one person followed me // automatically checked by #URL#',\n",
              " 'one person unfollowed me // automatically checked by #URL#',\n",
              " 'one person followed me and one person unfollowed me // automatically checked by #URL#',\n",
              " 'one person followed me // automatically checked by #URL#',\n",
              " 'one person unfollowed me // automatically checked by #URL#',\n",
              " 'one person followed me // automatically checked by #URL#',\n",
              " 'one person followed me // automatically checked by #URL#',\n",
              " 'one person followed me // automatically checked by #URL#',\n",
              " 'one person followed me // automatically checked by #URL#',\n",
              " 'one person followed me // automatically checked by #URL#',\n",
              " 'one person followed me // automatically checked by #URL#',\n",
              " 'one person followed me // automatically checked by #URL#',\n",
              " 'one person unfollowed me // automatically checked by #URL#',\n",
              " 'one person followed me // automatically checked by #URL#',\n",
              " 'one person followed me // automatically checked by #URL#',\n",
              " 'Trying see a 100 bands off this street shit and we going get it üíØ‚ùåüß¢',\n",
              " '4 people followed me // automatically checked by #URL#',\n",
              " 'one person unfollowed me // automatically checked by #URL#',\n",
              " 'one person followed me // automatically checked by #URL#',\n",
              " '#USER# Damn they give America girls a chance to have an exotic pussy üòÇüòÇüòÇüòÅ',\n",
              " '2 people followed me // automatically checked by #URL#',\n",
              " '#USER# Worst mistake of your life ü•∂üò≠',\n",
              " 'one person followed me // automatically checked by #URL#',\n",
              " 'one person unfollowed me // automatically checked by #URL#',\n",
              " 'I used to wanna be famous as long as I got this paper I can remain nameless üíØ #URL#',\n",
              " 'one person followed me // automatically checked by #URL#',\n",
              " 'one person followed me // automatically checked by #URL#',\n",
              " 'one person followed me // automatically checked by #URL#',\n",
              " 'one person followed me // automatically checked by #URL#',\n",
              " 'one person unfollowed me // automatically checked by #URL#',\n",
              " 'one person followed me // automatically checked by #URL#',\n",
              " 'one person followed me // automatically checked by #URL#',\n",
              " 'RT #USER#: League source: Former #HASHTAG# pick Hasheem Thabeet is working out for the Bucks next week! (Via #USER#)',\n",
              " 'one person unfollowed me // automatically checked by #URL#',\n",
              " 'one person followed me // automatically checked by #URL#',\n",
              " 'one person unfollowed me // automatically checked by #URL#',\n",
              " 'Shooting thus video next üíØ #URL#',\n",
              " 'one person unfollowed me // automatically checked by #URL#',\n",
              " '2 people followed me // automatically checked by #URL#',\n",
              " 'RT #USER#: Im getting back on my feet fuck niggas better watch out üíØ #URL#',\n",
              " 'Im getting back on my feet fuck niggas better watch out üíØ #URL#',\n",
              " 'one person followed me // automatically checked by #URL#',\n",
              " 'one person followed me and one person unfollowed me // automatically checked by #URL#',\n",
              " '2 people followed me // automatically checked by #URL#',\n",
              " 'one person unfollowed me // automatically checked by #URL#',\n",
              " 'one person followed me // automatically checked by #URL#',\n",
              " 'one person unfollowed me // automatically checked by #URL#',\n",
              " '3 people followed me // automatically checked by #URL#',\n",
              " 'one person unfollowed me // automatically checked by #URL#',\n",
              " '2 people followed me // automatically checked by #URL#',\n",
              " 'one person followed me and 2 people unfollowed me // automatically checked by #URL#',\n",
              " 'one person unfollowed me // automatically checked by #URL#',\n",
              " 'one person followed me // automatically checked by #URL#',\n",
              " 'one person unfollowed me // automatically checked by #URL#',\n",
              " 'one person unfollowed me // automatically checked by #URL#',\n",
              " 'one person followed me // automatically checked by #URL#',\n",
              " 'RT #USER#: Yeah this shit dropping soon rt and like if you enjoy #URL#',\n",
              " 'one person followed me // automatically checked by #URL#',\n",
              " 'one person unfollowed me // automatically checked by #URL#',\n",
              " 'one person unfollowed me // automatically checked by #URL#',\n",
              " 'one person followed me and one person unfollowed me // automatically checked by #URL#',\n",
              " '2 people followed me // automatically checked by #URL#',\n",
              " 'one person followed me // automatically checked by #URL#',\n",
              " 'one person followed me // automatically checked by #URL#',\n",
              " 'Yup we all dealing with it üò§üò≠ #URL#',\n",
              " '#USER# Keep going brodie ü§ôüíØüî•',\n",
              " 'one person followed me // automatically checked by #URL#',\n",
              " 'This is going to be a good year ü§ôüíØ #URL#',\n",
              " 'Thank you üíï #URL#',\n",
              " \"RT #USER#: It's to easy üòéüí™üèΩ #URL#\",\n",
              " 'If you 21 come show me some love tonight dm me for more infor #URL#',\n",
              " 'Not just a cheating but a lost generation in whole #URL#',\n",
              " 'RT #USER#: Wish a young nigga a happy birthday if you dont mind üòÖ',\n",
              " 'Wish a young nigga a happy birthday if you dont mind üòÖ',\n",
              " 'Whats it about? #URL#',\n",
              " '#USER# What the fuck she get hit with',\n",
              " 'Click the link in my bio ü§û‚úäüíØ #URL#',\n",
              " 'one person unfollowed me // automatically checked by #URL#',\n",
              " '#USER# Now look at him shit crazy aint it.... Stop telling on your self with music the feds listen',\n",
              " 'one person followed me and one person unfollowed me // automatically checked by #URL#',\n",
              " 'RT #USER#: Yeah 20 years later #HASHTAG# #URL#',\n",
              " 'Dont trust no nigga geeking for fame #URL#',\n",
              " 'Yeah 20 years later #HASHTAG# #URL#',\n",
              " '#USER# You i been doing this shit since ninth grade faxxxüíØ',\n",
              " 'Im performing live tonight if you in dc come out its going be litt #URL#',\n",
              " '#USER# birthday twin you gotta smoke with me on our gday üíØ',\n",
              " '2 people followed me // automatically checked by #URL#',\n",
              " '2 people followed me and one person unfollowed me // automatically checked by #URL#',\n",
              " 'one person followed me // automatically checked by #URL#',\n",
              " '#USER# hey people are complaining about not being able to cash out after making $150',\n",
              " 'one person unfollowed me // automatically checked by #URL#',\n",
              " 'one person followed me // automatically checked by #URL#',\n",
              " 'one person unfollowed me // automatically checked by #URL#',\n",
              " 'RT #USER#: -. we do walk ups we don‚Äôt drive byüòàü§´',\n",
              " 'RT #USER#: i need sex consistently.',\n",
              " 'one person unfollowed me // automatically checked by #URL#',\n",
              " 'one person unfollowed me // automatically checked by #URL#',\n",
              " 'RT #USER#: #USER# Sorry bro but we not rockin today',\n",
              " 'one person unfollowed me // automatically checked by #URL#',\n",
              " 'Happy birthday charity #URL#',\n",
              " 'RT #USER#: 100 RT nd I drop this weekend üî•üî•üíØfr doe #URL#',\n",
              " '2 people unfollowed me // automatically checked by #URL#',\n",
              " \"RT #USER#: Fuck all u bitch ass niggas that's watching ur man starve üíØüíØ\",\n",
              " 'one person followed me // automatically checked by #URL#',\n",
              " '#USER# Twitter is oc im gone üòÇüòÇüòÇüò≠',\n",
              " '#USER# Sign me up for the plant milk after that',\n",
              " 'Get them the fuck outta here üòÇüòÇüòÇüòÇ fuck ass steelers fans',\n",
              " '#USER# Fuck that shit',\n",
              " 'RT #USER#: Idk who need see this but STOP SMOKING NASTY ASS CIGARETTES!!! #URL#',\n",
              " 'RT #USER#: I‚Äôm not attracted to thot bitches...',\n",
              " 'RT #USER#: üòÇüòÇüòÇüòÇüòÇ yeah fuck that i rather face üíØ niggas jays dont look like mine #URL#',\n",
              " '2 people unfollowed me // automatically checked by #URL#',\n",
              " 'Go like and retweet üíØ #URL#',\n",
              " 'If you dont fuck with me cant get mad about what i do',\n",
              " '#USER# You looking mad gorgeous right there üòÇüòÅ',\n",
              " 'RT #USER#: Ask me what i been up too anit shit i been cooling 2019 coming soon that new justo dropping tune in',\n",
              " 'RT #USER#: If my brothers ever down bad they know i got em',\n",
              " 'Ask me what i been up too anit shit i been cooling 2019 coming soon that new justo dropping tune in #URL#',\n",
              " 'one person followed me // automatically checked by #URL#',\n",
              " 'When you think your finessing life üòÇ #URL#',\n",
              " 'one person followed me // automatically checked by #URL#',\n",
              " 'Not cole or herbo everybody else make room for justo #URL#',\n",
              " 'RT #USER#: We ain‚Äôt got time for the lames, we tryna live in the moment',\n",
              " 'You gotta perform #URL#',\n",
              " 'RT #USER#: Retweet for a financially stable 2019 #URL#',\n",
              " 'If your name dont start with a s and end with a e dont text me i only wat her',\n",
              " 'Lord knows i only want you stop faking and come get your real nigga you been waiting for',\n",
              " 'one person followed me // automatically checked by #URL#',\n",
              " 'RT #USER#: Da music y‚Äôall call ‚Äúcrankage‚Äù fucking sucks',\n",
              " 'Yea this my gang righ here twitter #URL#',\n",
              " 'RT #USER#: Only dope rappers/singers can RT this. :)',\n",
              " 'To everyone i fuck with keep working shit about to change üíØüíØüíØ',\n",
              " 'Go run them views up click the link in my bio',\n",
              " 'one person followed me and one person unfollowed me // automatically checked by #URL#',\n",
              " 'Some body tell simone i miss her',\n",
              " '#USER# Love for life bj soon as i make it on god ima make sure you come with me',\n",
              " 'RT #USER#: #USER# Proud of you bro. Wish I can make it',\n",
              " 'I got many more to come this just the begging #URL#',\n",
              " 'Out now link in my bio #URL#',\n",
              " 'Mood for tonight come support the kid #URL#']"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "training_set['tweets'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04744cd7-3042-4a55-b450-0e5d23eb0446",
      "metadata": {
        "id": "04744cd7-3042-4a55-b450-0e5d23eb0446",
        "outputId": "bd9999cc-9dd6-42a6-f094-69a808fcc7a6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-21 10:46:01.535250: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: SSE4.1 SSE4.2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import datetime\n",
        "import torch\n",
        "import random\n",
        "# import matplotlib.pyplot as plt\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "plt.style.use('fivethirtyeight')\n",
        "\n",
        "class torch_model(object):\n",
        "    def __init__(self, model, loss_fn, optimizer):\n",
        "        # Here we define the attributes of our class\n",
        "\n",
        "        # We start by storing the arguments as attributes\n",
        "        # to use them later\n",
        "        self.model = model\n",
        "        self.loss_fn = loss_fn\n",
        "        self.optimizer = optimizer\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        # Let's send the model to the specified device right away\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        # These attributes are defined here, but since they are\n",
        "        # not informed at the moment of creation, we keep them None\n",
        "        self.train_loader = None\n",
        "        self.val_loader = None\n",
        "        self.writer = None\n",
        "\n",
        "        # These attributes are going to be computed internally\n",
        "        self.losses = []\n",
        "        self.val_losses = []\n",
        "        self.total_epochs = 0\n",
        "\n",
        "        # Creates the train_step function for our model,\n",
        "        # loss function and optimizer\n",
        "        # Note: there are NO ARGS there! It makes use of the class\n",
        "        # attributes directly\n",
        "        self.train_step_fn = self._make_train_step_fn()\n",
        "        # Creates the val_step function for our model and loss\n",
        "        self.val_step_fn = self._make_val_step_fn()\n",
        "\n",
        "    def to(self, device):\n",
        "        # This method allows the user to specify a different device\n",
        "        # It sets the corresponding attribute (to be used later in\n",
        "        # the mini-batches) and sends the model to the device\n",
        "        try:\n",
        "            self.device = device\n",
        "            self.model.to(self.device)\n",
        "        except RuntimeError:\n",
        "            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "            print(f\"Couldn't send it to {device}, sending it to {self.device} instead.\")\n",
        "            self.model.to(self.device)\n",
        "\n",
        "    def set_loaders(self, train_loader, val_loader=None):\n",
        "        # This method allows the user to define which train_loader (and val_loader, optionally) to use\n",
        "        # Both loaders are then assigned to attributes of the class\n",
        "        # So they can be referred to later\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "\n",
        "    def set_tensorboard(self, name, folder='runs'):\n",
        "        # This method allows the user to define a SummaryWriter to interface with TensorBoard\n",
        "        suffix = datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n",
        "        self.writer = SummaryWriter(f'{folder}/{name}_{suffix}')\n",
        "\n",
        "    def _make_train_step_fn(self):\n",
        "        # This method does not need ARGS... it can refer to\n",
        "        # the attributes: self.model, self.loss_fn and self.optimizer\n",
        "\n",
        "        # Builds function that performs a step in the train loop\n",
        "        def perform_train_step_fn(x, y):\n",
        "            # Sets model to TRAIN mode\n",
        "            self.model.train()\n",
        "\n",
        "            # Step 1 - Computes our model's predicted output - forward pass\n",
        "            yhat = self.model(x)\n",
        "            # Step 2 - Computes the loss\n",
        "            loss = self.loss_fn(yhat, y)\n",
        "            # Step 3 - Computes gradients for both \"a\" and \"b\" parameters\n",
        "            loss.backward()\n",
        "            # Step 4 - Updates parameters using gradients and the learning rate\n",
        "            self.optimizer.step()\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            # Returns the loss\n",
        "            return loss.item()\n",
        "\n",
        "        # Returns the function that will be called inside the train loop\n",
        "        return perform_train_step_fn\n",
        "\n",
        "    def _make_val_step_fn(self):\n",
        "        # Builds function that performs a step in the validation loop\n",
        "        def perform_val_step_fn(x, y):\n",
        "            # Sets model to EVAL mode\n",
        "            self.model.eval()\n",
        "\n",
        "            # Step 1 - Computes our model's predicted output - forward pass\n",
        "            yhat = self.model(x)\n",
        "            # Step 2 - Computes the loss\n",
        "            loss = self.loss_fn(yhat, y)\n",
        "            # There is no need to compute Steps 3 and 4, since we don't update parameters during evaluation\n",
        "            return loss.item()\n",
        "\n",
        "        return perform_val_step_fn\n",
        "\n",
        "    def _mini_batch(self, validation=False):\n",
        "        # The mini-batch can be used with both loaders\n",
        "        # The argument `validation`defines which loader and\n",
        "        # corresponding step function is going to be used\n",
        "        if validation:\n",
        "            data_loader = self.val_loader\n",
        "            step_fn = self.val_step_fn\n",
        "        else:\n",
        "            data_loader = self.train_loader\n",
        "            step_fn = self.train_step_fn\n",
        "\n",
        "        if data_loader is None:\n",
        "            return None\n",
        "\n",
        "        # Once the data loader and step function, this is the same\n",
        "        # mini-batch loop we had before\n",
        "        mini_batch_losses = []\n",
        "        for x_batch, y_batch in data_loader:\n",
        "            x_batch = x_batch.to(self.device)\n",
        "            y_batch = y_batch.to(self.device)\n",
        "\n",
        "            mini_batch_loss = step_fn(x_batch, y_batch)\n",
        "            mini_batch_losses.append(mini_batch_loss)\n",
        "\n",
        "        loss = np.mean(mini_batch_losses)\n",
        "        return loss\n",
        "\n",
        "    def set_seed(self, seed=42):\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "        torch.manual_seed(seed)\n",
        "        np.random.seed(seed)\n",
        "        random.seed(seed)\n",
        "        try:\n",
        "            self.train_loader.sampler.generator.manual_seed(seed)\n",
        "        except AttributeError:\n",
        "            pass\n",
        "\n",
        "    def train(self, n_epochs, seed=42):\n",
        "        # To ensure reproducibility of the training process\n",
        "        self.set_seed(seed)\n",
        "\n",
        "        for epoch in range(n_epochs):\n",
        "            # Keeps track of the numbers of epochs\n",
        "            # by updating the corresponding attribute\n",
        "            self.total_epochs += 1\n",
        "\n",
        "            # inner loop\n",
        "            # Performs training using mini-batches\n",
        "            loss = self._mini_batch(validation=False)\n",
        "            self.losses.append(loss)\n",
        "\n",
        "            # VALIDATION\n",
        "            # no gradients in validation!\n",
        "            with torch.no_grad():\n",
        "                # Performs evaluation using mini-batches\n",
        "                val_loss = self._mini_batch(validation=True)\n",
        "                self.val_losses.append(val_loss)\n",
        "\n",
        "            # If a SummaryWriter has been set...\n",
        "            if self.writer:\n",
        "                scalars = {'training': loss}\n",
        "                if val_loss is not None:\n",
        "                    scalars.update({'validation': val_loss})\n",
        "                # Records both losses for each epoch under the main tag \"loss\"\n",
        "                self.writer.add_scalars(main_tag='loss',\n",
        "                                        tag_scalar_dict=scalars,\n",
        "                                        global_step=epoch)\n",
        "\n",
        "        if self.writer:\n",
        "            # Closes the writer\n",
        "            self.writer.close()\n",
        "\n",
        "    def save_checkpoint(self, filename):\n",
        "        # Builds dictionary with all elements for resuming training\n",
        "        checkpoint = {'epoch': self.total_epochs,\n",
        "                      'model_state_dict': self.model.state_dict(),\n",
        "                      'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "                      'loss': self.losses,\n",
        "                      'val_loss': self.val_losses}\n",
        "\n",
        "        torch.save(checkpoint, filename)\n",
        "\n",
        "    def load_checkpoint(self, filename):\n",
        "        # Loads dictionary\n",
        "        checkpoint = torch.load(filename, weights_only=False)\n",
        "\n",
        "        # Restore state for model and optimizer\n",
        "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "        self.total_epochs = checkpoint['epoch']\n",
        "        self.losses = checkpoint['loss']\n",
        "        self.val_losses = checkpoint['val_loss']\n",
        "\n",
        "        self.model.train() # always use TRAIN for resuming training\n",
        "\n",
        "    def predict(self, x):\n",
        "        # Set is to evaluation mode for predictions\n",
        "        self.model.eval()\n",
        "        # Takes aNumpy input and make it a float tensor\n",
        "        x_tensor = torch.as_tensor(x).float()\n",
        "        # Send input to device and uses model for prediction\n",
        "        y_hat_tensor = self.model(x_tensor.to(self.device))\n",
        "        # Set it back to train mode\n",
        "        self.model.train()\n",
        "        # Detaches it, brings it to CPU and back to Numpy\n",
        "        return y_hat_tensor.detach().cpu().numpy()\n",
        "\n",
        "    def plot_losses(self):\n",
        "        fig = plt.figure(figsize=(10, 4))\n",
        "        plt.plot(self.losses, label='Training Loss', c='b')\n",
        "        plt.plot(self.val_losses, label='Validation Loss', c='r')\n",
        "        plt.yscale('log')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        return fig\n",
        "\n",
        "    def add_graph(self):\n",
        "        # Fetches a single mini-batch so we can use add_graph\n",
        "        if self.train_loader and self.writer:\n",
        "            x_sample, y_sample = next(iter(self.train_loader))\n",
        "            self.writer.add_graph(self.model, x_sample.to(self.device))\n",
        "\n",
        "    def count_parameters(self):\n",
        "        return sum(p.numel() for p in self.model.parameters())\n",
        "\n",
        "    def count_trainable_parameters(self):\n",
        "        return sum(p.numel() for p in self.model.parameters() if p.requires_grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "882a369c-c841-4abb-994d-88009db9143e",
      "metadata": {
        "id": "882a369c-c841-4abb-994d-88009db9143e"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "prueba = [[1,2,3],[4,5,6]]\n",
        "copia_prueba = [[],[]]\n",
        "copia_prueba[0].append(prueba[0][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5508035-9259-4c2d-8eeb-5f0fc8ab2cca",
      "metadata": {
        "id": "c5508035-9259-4c2d-8eeb-5f0fc8ab2cca",
        "outputId": "4c9e7ceb-0cfc-4e50-b094-aeb9541f56b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1, 2, 3], [4, 5, 6]]\n",
            "[[1], []]\n",
            "[['xd', 2, 3], [4, 5, 6]]\n",
            "[[1], []]\n"
          ]
        }
      ],
      "source": [
        "print(prueba)\n",
        "print(copia_prueba)\n",
        "prueba[0][0] = 'xd'\n",
        "print(prueba)\n",
        "print(copia_prueba)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92be0345-7849-475f-adbb-465fc7f9e39d",
      "metadata": {
        "id": "92be0345-7849-475f-adbb-465fc7f9e39d",
        "outputId": "27662deb-e738-4f36-faf1-b7e90c670134"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "400"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(training_set['lc_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37661704-7035-49ea-acdb-264fd0e3cc35",
      "metadata": {
        "id": "37661704-7035-49ea-acdb-264fd0e3cc35",
        "outputId": "a081a4ea-d8a3-4a77-f389-b77df40e9465"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user</th>\n",
              "      <th>tweets</th>\n",
              "      <th>language</th>\n",
              "      <th>class</th>\n",
              "      <th>lc_text</th>\n",
              "      <th>TBW_tok_text</th>\n",
              "      <th>TBW_tok_cln_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>d2e0f4f0244b9b8b3bbd8b1654be5b74.xml</td>\n",
              "      <td>[I‚Äôm just being me under construction #HASHTAG...</td>\n",
              "      <td>english</td>\n",
              "      <td>0</td>\n",
              "      <td>[i‚Äôm just being me under construction #hashtag...</td>\n",
              "      <td>[[i‚Äôm, just, being, me, under, construction, #...</td>\n",
              "      <td>[[i‚Äôm, construction, #, hashtag, #, way, #, ur...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>f30abea44b6c4144c0690f98459428a6.xml</td>\n",
              "      <td>[i was locked up sending you roses, acabei de ...</td>\n",
              "      <td>english</td>\n",
              "      <td>0</td>\n",
              "      <td>[i was locked up sending you roses, acabei de ...</td>\n",
              "      <td>[[i, was, locked, up, sending, you, roses], [a...</td>\n",
              "      <td>[[locked, sending, roses], [acabei, de, ver, g...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>d28b60028cf7bcf8a9f145193e261ecf.xml</td>\n",
              "      <td>[This crippled goob saw the ‚Äúanti-Semitic plat...</td>\n",
              "      <td>english</td>\n",
              "      <td>1</td>\n",
              "      <td>[this crippled goob saw the ‚Äúanti-semitic plat...</td>\n",
              "      <td>[[this, crippled, goob, saw, the, ‚Äúanti-semiti...</td>\n",
              "      <td>[[crippled, goob, saw, ‚Äúanti-semitic, platform...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>78537787441ed0d11da43122f9b0520a.xml</td>\n",
              "      <td>[RT #USER#: EXCLUSIVE: 99% chance COVID-19 was...</td>\n",
              "      <td>english</td>\n",
              "      <td>1</td>\n",
              "      <td>[rt #user#: exclusive: 99% chance covid-19 was...</td>\n",
              "      <td>[[rt, #, user, #, :, exclusive, :, 99, %, chan...</td>\n",
              "      <td>[[rt, #, user, #, :, exclusive, :, 99, %, chan...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>30be488aa93e8000aadb952a9cd5143c.xml</td>\n",
              "      <td>[#USER# Yes, yes she did, RT #USER#: What are ...</td>\n",
              "      <td>english</td>\n",
              "      <td>0</td>\n",
              "      <td>[#user# yes, yes she did, rt #user#: what are ...</td>\n",
              "      <td>[[#, user, #, yes, ,, yes, she, did], [rt, #, ...</td>\n",
              "      <td>[[#, user, #, yes, ,, yes], [rt, #, user, #, :...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   user  \\\n",
              "0  d2e0f4f0244b9b8b3bbd8b1654be5b74.xml   \n",
              "1  f30abea44b6c4144c0690f98459428a6.xml   \n",
              "2  d28b60028cf7bcf8a9f145193e261ecf.xml   \n",
              "3  78537787441ed0d11da43122f9b0520a.xml   \n",
              "4  30be488aa93e8000aadb952a9cd5143c.xml   \n",
              "\n",
              "                                              tweets language  class  \\\n",
              "0  [I‚Äôm just being me under construction #HASHTAG...  english      0   \n",
              "1  [i was locked up sending you roses, acabei de ...  english      0   \n",
              "2  [This crippled goob saw the ‚Äúanti-Semitic plat...  english      1   \n",
              "3  [RT #USER#: EXCLUSIVE: 99% chance COVID-19 was...  english      1   \n",
              "4  [#USER# Yes, yes she did, RT #USER#: What are ...  english      0   \n",
              "\n",
              "                                             lc_text  \\\n",
              "0  [i‚Äôm just being me under construction #hashtag...   \n",
              "1  [i was locked up sending you roses, acabei de ...   \n",
              "2  [this crippled goob saw the ‚Äúanti-semitic plat...   \n",
              "3  [rt #user#: exclusive: 99% chance covid-19 was...   \n",
              "4  [#user# yes, yes she did, rt #user#: what are ...   \n",
              "\n",
              "                                        TBW_tok_text  \\\n",
              "0  [[i‚Äôm, just, being, me, under, construction, #...   \n",
              "1  [[i, was, locked, up, sending, you, roses], [a...   \n",
              "2  [[this, crippled, goob, saw, the, ‚Äúanti-semiti...   \n",
              "3  [[rt, #, user, #, :, exclusive, :, 99, %, chan...   \n",
              "4  [[#, user, #, yes, ,, yes, she, did], [rt, #, ...   \n",
              "\n",
              "                                    TBW_tok_cln_text  \n",
              "0  [[i‚Äôm, construction, #, hashtag, #, way, #, ur...  \n",
              "1  [[locked, sending, roses], [acabei, de, ver, g...  \n",
              "2  [[crippled, goob, saw, ‚Äúanti-semitic, platform...  \n",
              "3  [[rt, #, user, #, :, exclusive, :, 99, %, chan...  \n",
              "4  [[#, user, #, yes, ,, yes], [rt, #, user, #, :...  "
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#print(tokenizer.tokenize_sents(training_set['lc_text'][0]))\n",
        "#filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
        "\n",
        "#training_set.head()\n",
        "\n",
        "import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3beb405-7497-45dc-ab45-8fea8fdd4f9b",
      "metadata": {
        "id": "d3beb405-7497-45dc-ab45-8fea8fdd4f9b",
        "outputId": "24204842-9fde-450e-ffb9-5b41e21617dc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"\\nfor user in training_set['tweets']:\\n    user_list = list()\\n    for tweet in user:\\n        user_list.append(tweet.lower())\\n    aux_list.append(user_list)\\ntraining_set['lc_text'] = aux_list\\ntraining_set['TBW_tok_text'] = copy.deepcopy(training_set['lc_text'])\\n\""
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Tokenize text\n",
        "\n",
        "import nltk\n",
        "\n",
        "from nltk.tokenize import TreebankWordTokenizer, word_tokenize, sent_tokenize, TweetTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "TBW_tokenizer = TreebankWordTokenizer()\n",
        "tweet_tokenizer = TweetTokenizer()\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "#filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
        "#print(tokenizer.tokenize_sents(training_set['tweets'][0]))\n",
        "\n",
        "#aux_list = list()\n",
        "num_users = training_set.shape[0]\n",
        "num_tweets_eahc_user = [ len(training_set['tweets'][i]) for i in range(num_users) ]\n",
        "\n",
        "\"\"\"\n",
        "for i in range(num_users):\n",
        "    aux_list.append([])\n",
        "    for j in range(num_tweets_eahc_user[i]):\n",
        "        aux_list[i].append(None)\n",
        "\"\"\"\n",
        "\n",
        "training_set['lc_text'] = [ [] for i in range(num_users) ]\n",
        "training_set['TBW_tok_text'] = [ [] for i in range(num_users) ]\n",
        "training_set['TBW_tok_cln_text'] = [ [] for i in range(num_users) ]\n",
        "training_set['tweet_tok_text'] = [ [] for i in range(num_users) ]\n",
        "training_set['tweet_tok_cln_text'] = [ [] for i in range(num_users) ]\n",
        "training_set['word_tok_text'] = [ [] for i in range(num_users) ]\n",
        "training_set['word_tok_cln_text'] = [ [] for i in range(num_users) ]\n",
        "\n",
        "for i in range(num_users):\n",
        "    for j in range(num_tweets_eahc_user[i]):\n",
        "        training_set['lc_text'][i].append(training_set['tweets'][i][j].lower())\n",
        "        training_set['TBW_tok_text'][i].append(TBW_tokenizer.tokenize(training_set['lc_text'][i][j]))\n",
        "        training_set['tweet_tok_text'][i].append(tweet_tokenizer.tokenize(training_set['lc_text'][i][j]))\n",
        "        training_set['word_tok_text'][i].append(word_tokenize(training_set['lc_text'][i][j]))\n",
        "    for j in range(num_tweets_eahc_user[i]):\n",
        "        training_set['TBW_tok_cln_text'][i].append([])\n",
        "        training_set['tweet_tok_cln_text'][i].append([])\n",
        "        training_set['word_tok_cln_text'][i].append([])\n",
        "        for token in training_set['TBW_tok_text'][i][j]:\n",
        "            if not token in stop_words:\n",
        "                training_set['TBW_tok_cln_text'][i][j].append(token)\n",
        "                training_set['tweet_tok_cln_text'][i][j].append(token)\n",
        "                training_set['word_tok_cln_text'][i][j].append(token)\n",
        "\n",
        "\"\"\"\n",
        "for user in training_set['tweets']:\n",
        "    user_list = list()\n",
        "    for tweet in user:\n",
        "        user_list.append(tweet.lower())\n",
        "    aux_list.append(user_list)\n",
        "training_set['lc_text'] = aux_list\n",
        "training_set['TBW_tok_text'] = copy.deepcopy(training_set['lc_text'])\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "#print('--------------------------------------------------------')\n",
        "\n",
        "#tokenizer = TweetTokenizer()\n",
        "\n",
        "#print(tokenizer.tokenize_sents(training_set['tweets'][0]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e43637ea-bc37-48a1-a993-aab7b2825890",
      "metadata": {
        "id": "e43637ea-bc37-48a1-a993-aab7b2825890",
        "outputId": "91154dba-2430-42d7-ce3b-02aaad5563e0"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'X' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[10], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataLoader, TensorDataset, random_split\n\u001b[1;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential()\n\u001b[0;32m----> 8\u001b[0m model\u001b[38;5;241m.\u001b[39madd_module(nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[43mX\u001b[49m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m512\u001b[39m))\n\u001b[1;32m      9\u001b[0m model\u001b[38;5;241m.\u001b[39madd_module(nn\u001b[38;5;241m.\u001b[39mReLU())\n\u001b[1;32m     10\u001b[0m model\u001b[38;5;241m.\u001b[39madd_module(nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m512\u001b[39m))\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "\n",
        "model = nn.Sequential()\n",
        "\n",
        "model.add_module(nn.Linear(X.shape[0], 512))\n",
        "model.add_module(nn.ReLU())\n",
        "model.add_module(nn.Linear(512, 512))\n",
        "model.add_module(nn.ReLU())\n",
        "model.add_module(nn.Linear(512, 1))\n",
        "\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92b4f051-8372-46c2-b582-1bcdba7a4e3f",
      "metadata": {
        "id": "92b4f051-8372-46c2-b582-1bcdba7a4e3f"
      },
      "outputs": [],
      "source": [
        "torch.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f2b9210-4a8b-4997-8d35-f6cea9e69f38",
      "metadata": {
        "id": "4f2b9210-4a8b-4997-8d35-f6cea9e69f38"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}